# ðŸ¤– Danish Transformers 

Transformers constitute the current paradigm within Natural Language Processing (NLP) for a variety of downstream tasks. 
The number of transformers trained on danish corpora are limited, which is why the ambition of this repository is to provide the danish NLP community with alternatives to already established models. 
The pretrained models in this repository are trained using [PyTorch](https://pytorch.org/) and [ðŸ¤—Transformers](https://github.com/huggingface/transformers), 
and checkpoints are made available at the HuggingFace model hub [here](https://huggingface.co/sarnikowski) for both PyTorch and [TensorFlow](https://www.tensorflow.org/).

## Architectures

### [ELECTRA](electra/README.md)

## License

[![CC BY 4.0][cc-by-image]][cc-by]

This work is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].

[cc-by]: http://creativecommons.org/licenses/by/4.0/
[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png

